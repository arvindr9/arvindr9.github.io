---
layout: post
title:  "Boosting"
date:   2022-07-12 18:10:00 -0400
categories: blog
---

{% include_relative posts.html %}


<!-- Outline:
- Intro
- Multiplicative weights algorithm
- Boosting algo and proof
- Gradient boosting (maybe in another post?) -->


We will discuss boosting from the perspective of the multiplicative weights framework.

## Multiplicative weights framework

Suppose you are given $$T$$ timesteps, and you have $$n$$ actions to pick from at each timestep.
For each timestep, you suffer a cost based on your action. We do not know what the cost will be until we
pick that action.

What is it that we want to achieve? We cannot fully get the smallest cost possible since we
do not have the power to tell the costs of future actions.

We can model this as a problem with $$n$$ experts, where each expert picks the same action at each timestep. Denote $$m_i^{(t)}$$ to be the cost incurred by taking action $$i$$ at timestep $$t$$.
We don't want our cost to be too much worse than that of the best expert. The difference between the cost of the algorithm and the cost of the best expert is often referred to as _regret_.

Consider the following algorithm:

Set $$w_i^{(0)} = 1$$ for $$i = 1...n$$

For $$t = 1...T$$:

Pick expert $$i$$ with probabilty $$p_i^{(t)} = \frac{w_i^{(t-1)}}{\Phi^{t-1}}$$ where $$\Phi^{t-1} = \sum_{i=1}^n w_{i}^{(t-1)}$$

Observe costs of experts, and set $$w_i^{(t)} := w_i^{(t-1)}(1 - \eta m_i^{(t)})$$ for all $$i$$.


The expected cost of the algorithm will by $$sum_{t=1}^T m^{(t)} \cdot p^{(t)}$$.

The algorithm has the following guarantee:

After $$T$$ rounds, for any $$i$$,

$$sum_{t=1}^T m^{(t)} \cdot p^{(t)} \leq sum_{t=1}^T (m^{(t)} + \eta|m^{(t)}|) \cdot p + \frac{\ln n}{\eta}$$

Proof:

We proceed by finding upper and lower bounds for $$\Phi^(T)$$.

We have that for any $$i$$, $$w_i^{(T)} \leq Phi^{(T)}$$ and $$w_i{(T)} = \prod_{t=1}^T (1 - \eta m_{i}^{(t)})$$, so 
$$(1 - \eta m_{i}^{(t)}) \leq \Phi^(T)$$.

For the other direction, we have that

$$\Phi^{(T)} = \sum_{t = 1}}^T w_{i}^{(T)} = \sum_{t=1}^T (1 - \eta m_i^{(T)}) w_{i}^{(T-1)}$$

$$= \Phi^{(T)} - \eta \Phi^{(T)} \sum_{t=1}^{T} m_i^{(t)} p_i^{(t)}$$

$$= \Phi^{(T)} (1 - \eta m^{(t)} \cdot p^{(t)})$$

$$\leq \Phi^{(T)} \exp(-\eta m^{(t)} \cdot p^{(t)})$$

With some amount of further algebra from the upper and lower bounds, we can arrive at the guarantee (mentioned in [1])


<!-- TODO: complete the proof of this bound -->



## Weak learning

Let's assume that we have a "weak learning" algorithm; i.e. one that can be trained on a finite sample of distribution
and can achieve error $$\frac{1}{2} - \gamma$$ on that distribution.

## Guarantees for strong learning through multiplicative weights


## Gradient Boosting

## References

[1] The Multiplicative Weights Update Method: A Meta-Algorithm and Applications
